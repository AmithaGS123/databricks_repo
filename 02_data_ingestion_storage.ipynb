{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c28cdc8-5f36-432d-8a2d-cb4e8c95f3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Module 02 - Data Ingestion and Storage\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers data ingestion and storage in Databricks. You'll learn how to work with DBFS, integrate with Azure Data Lake Storage Gen2, and read/write various file formats.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will understand:\n",
    "- Working with Databricks File System (DBFS)\n",
    "- Mounting external storage (ADLS Gen2)\n",
    "- Reading and writing different file formats (CSV, JSON, Parquet, etc.)\n",
    "- Best practices for data storage in Databricks\n",
    "- Working with large datasets efficiently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837fad6c-4d79-4895-ba8d-2ad1f5b0cfe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Understanding DBFS (Databricks File System)\n",
    "\n",
    "DBFS is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. It's an abstraction layer over cloud storage that makes it easy to work with files.\n",
    "\n",
    "### DBFS Structure\n",
    "\n",
    "- `/` - Root directory\n",
    "- `/FileStore` - Files uploaded through UI or generated outputs\n",
    "- `/databricks` - System files and libraries\n",
    "- `/tmp` - Temporary files\n",
    "- `/mnt` - Mount points for external storage (ADLS, S3, etc.)\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Unified Interface**: Same API for local and cloud storage\n",
    "2. **Performance**: Optimized for Spark workloads\n",
    "3. **Persistence**: Files persist across cluster restarts\n",
    "4. **Access Control**: Integrated with Databricks security\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d06e68-c96d-4af5-9666-22adb0047e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore DBFS structure\n",
    "print(\"DBFS Root Contents:\")\n",
    "for item in dbutils.fs.ls(\"/\"):\n",
    "    print(f\"  {item.name:20s} - Size: {item.size} bytes, Modified: {item.modificationTime}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e2c5c6-4c58-457f-a629-fd7b1a7ac432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a19cbaa-2901-4bcc-8517-17ca290d66b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS databricks_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461d2874-e344-4f6b-981f-4eb05ef0cb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a directory structure for our demo\n",
    "demo_path = \"/Volumes/workspace/default/databricks_demo\"\n",
    "\n",
    "# List contents\n",
    "print(f\"\\nContents of {demo_path}:\")\n",
    "for item in dbutils.fs.ls(demo_path):\n",
    "    print(f\"  {item.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e58e1cc-b46a-498e-a1fe-94742337a8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use your existing Volume path\n",
    "# Format: /Volumes/<catalog>/<schema>/<volume_name>/<folder>\n",
    "demo_path = \"/Volumes/workspace/default/databricks_demo\"\n",
    "\n",
    "# Create the directory inside the Volume\n",
    "dbutils.fs.mkdirs(demo_path)\n",
    "print(f\"Created directory in Volume: {demo_path}\")\n",
    "\n",
    "# List contents\n",
    "print(f\"\\nContents of {demo_path}:\")\n",
    "for item in dbutils.fs.ls(demo_path):\n",
    "    print(f\"  {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7245f2d1-bd02-42b2-91c0-ed3153144a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with FileStore\n",
    "\n",
    "FileStore is the default location for files uploaded through the Databricks UI. It's accessible via `/FileStore` in DBFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa141a56-091f-4e67-96f8-a42e4b364e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data and save to FileStore\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create sample DataFrame\n",
    "sample_data = {\n",
    "    'id': range(1, 101),\n",
    "    'name': [f'User_{i}' for i in range(1, 101)],\n",
    "    'age': [20 + (i % 40) for i in range(1, 101)],\n",
    "    'city': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'] * 20,\n",
    "    'salary': [50000 + (i * 1000) for i in range(1, 101)]\n",
    "}\n",
    "\n",
    "pandas_df = pd.DataFrame(sample_data)\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Save to FileStore\n",
    "output_path = \"/Volumes/workspace/default/databricks_demo/sample_users.parquet\"\n",
    "spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Data saved to: {output_path}\")\n",
    "\n",
    "# Verify the file exists\n",
    "files = dbutils.fs.ls(\"/Volumes/workspace/default/databricks_demo\")\n",
    "print(f\"\\n/Volumes/workspace/default/databricks_demo:\")\n",
    "for file in files:\n",
    "    if \"sample_users\" in file.name:\n",
    "        print(f\"  {file.name} - {file.size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa03a12-730d-4c41-a1ad-9aafdad99611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mounting Azure Data Lake Storage Gen2\n",
    "\n",
    "Mounting ADLS Gen2 allows you to access data as if it were on the local file system. This is the recommended way to access external storage in Databricks.\n",
    "\n",
    "### Prerequisites for Mounting\n",
    "\n",
    "1. **Storage Account**: Your ADLS Gen2 storage account name\n",
    "2. **Container**: The container name in your storage account\n",
    "3. **Authentication**: Service Principal or Access Key\n",
    "\n",
    "### Mount Process\n",
    "\n",
    "1. Configure authentication (Service Principal recommended)\n",
    "2. Create mount point using dbutils.fs.mount()\n",
    "3. Access data via `/mnt/mount_name/...`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ca85c55-7578-47e4-a82a-88edadfe42e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Add secret scope here:\n",
    "\n",
    "https://<<youraccount>>.cloud.databricks.com/#secrets/createScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4453efe0-f9b6-4923-891d-dc332793483f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuration (Replace with your actual values)\n",
    "storage_account_name = \"cetpastorage\"\n",
    "container_name = \"practice-dataset\"\n",
    "# client_id = dbutils.secrets.get(scope=\"adls-scope\", key=\"adls-sp-client-id\")\n",
    "# client_secret = dbutils.secrets.get(scope=\"adls-scope\", key=\"adls-sp-client-secret\")\n",
    "# tenant_id = dbutils.secrets.get(scope=\"adls-scope\", key=\"adls-sp-tenant-id\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Set Spark Configuration directly for this session\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# 3. Access the data directly using the ABFSS path\n",
    "path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "print(\"Listing files in ADLS Gen2:\")\n",
    "display(dbutils.fs.ls(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a6ceb2-a3be-40b5-b99a-a7d64dbf7db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"cetpastorage\"\n",
    "container_name = \"practice-dataset\"\n",
    "\n",
    "\n",
    "\n",
    "# Define the connection options as a dictionary\n",
    "adls_options = {\n",
    "  \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "  \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "  \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "}\n",
    "\n",
    "# The Direct Path\n",
    "path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# 2026 Recommended Method: Passing options directly to the reader\n",
    "# Replace 'csv' with 'parquet' or 'json' as needed\n",
    "try:\n",
    "    df = (spark.read\n",
    "          .format(\"csv\") \n",
    "          .options(**adls_options) # This injects your credentials for THIS read only\n",
    "          .load(path + \"your_file.csv\"))\n",
    "    \n",
    "    display(df)\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe687d76-3d4e-4689-b09f-1ee48e43236c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 1: Mounting ADLS Gen2 using Service Principal (Recommended)\n",
    "# This is a template - replace with your actual values\n",
    "\n",
    "# Configuration\n",
    "storage_account_name = \"cetpastorage\"\n",
    "container_name = \"practice-dataset\"\n",
    "mount_point = \"/mnt/adls_demo\"\n",
    "\n",
    "# Get secrets from Databricks Secret Scope\n",
    "# Make sure to create a secret scope with these keys:\n",
    "# - adls-sp-client-id\n",
    "# - adls-sp-client-secret\n",
    "# - adls-sp-tenant-id\n",
    "\n",
    "try:\n",
    "    # Check if already mounted\n",
    "    if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "        print(f\"{mount_point} is already mounted\")\n",
    "    else:\n",
    "        # Get credentials from secret scope\n",
    "        \n",
    "        \n",
    "        # Configure Spark\n",
    "        configs = {\n",
    "            \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "            \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "            \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "            \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "        \n",
    "        # Mount the storage\n",
    "        source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "        dbutils.fs.mount(\n",
    "            source=source,\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        print(f\"Successfully mounted {source} to {mount_point}\")\n",
    "except Exception as e:\n",
    "    print(f\"Mounting failed (this is expected if credentials are not configured): {e}\")\n",
    "    print(\"\\nTo mount ADLS Gen2, you need to:\")\n",
    "    print(\"1. Create a Databricks Secret Scope\")\n",
    "    print(\"2. Add Service Principal credentials to the scope\")\n",
    "    print(\"3. Update the storage_account_name and container_name variables\")\n",
    "    print(\"4. Run this cell again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "becaacd0-1d42-4cfb-996b-a97dea184219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 2: Mounting using Access Key (Alternative method)\n",
    "# Note: Access keys are less secure than Service Principal\n",
    "\n",
    "storage_account_name = \"YOUR_STORAGE_ACCOUNT_NAME\"\n",
    "container_name = \"YOUR_CONTAINER_NAME\"\n",
    "mount_point = \"/mnt/adls_access_key\"\n",
    "\n",
    "try:\n",
    "    # Get access key from secret scope\n",
    "    access_key = dbutils.secrets.get(scope=\"adls-scope\", key=\"adls-access-key\")\n",
    "    \n",
    "    configs = {\n",
    "        \"fs.azure.account.key.\" + storage_account_name + \".dfs.core.windows.net\": access_key\n",
    "    }\n",
    "    \n",
    "    source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "    \n",
    "    if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "        print(f\"{mount_point} is already mounted\")\n",
    "    else:\n",
    "        dbutils.fs.mount(\n",
    "            source=source,\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        print(f\"Successfully mounted using access key\")\n",
    "except Exception as e:\n",
    "    print(f\"Mounting with access key failed: {e}\")\n",
    "    print(\"This is expected if credentials are not configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff8e709-4fb1-4e92-8225-ae754d12c137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all current mounts\n",
    "print(\"Current Mount Points:\")\n",
    "mounts = dbutils.fs.mounts()\n",
    "for mount in mounts:\n",
    "    print(f\"  {mount.mountPoint:30s} -> {mount.source}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "281e9820-af2c-49dc-8396-efcfa3a1f181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Direct Access to ADLS Gen2 (Without Mounting)\n",
    "\n",
    "You can also access ADLS Gen2 directly without mounting. This is useful for one-time access or when you don't want to maintain mount points.\n",
    "\n",
    "### Direct Access Methods\n",
    "\n",
    "1. **Using abfss:// protocol** - Azure Blob File System Secure\n",
    "2. **Using spark.conf** - Configure authentication at Spark level\n",
    "3. **Using dbutils.secrets** - Secure credential management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b10079-4717-41fa-ac85-253065f08bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Direct access to ADLS Gen2 using Service Principal\n",
    "# Configure at SparkSession level\n",
    "\n",
    "storage_account_name = \"YOUR_STORAGE_ACCOUNT_NAME\"\n",
    "container_name = \"YOUR_CONTAINER_NAME\"\n",
    "\n",
    "# Get credentials from secret scope\n",
    "try:\n",
    "    \n",
    "    \n",
    "    # Configure Spark for direct access\n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "                  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\",\n",
    "                  f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "    \n",
    "    # Now you can read directly\n",
    "    direct_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/path/to/your/file.parquet\"\n",
    "    print(f\"Configured for direct access. Use path: {direct_path}\")\n",
    "    print(\"Example: df = spark.read.parquet(direct_path)\")\n",
    "except Exception as e:\n",
    "    print(f\"Configuration failed: {e}\")\n",
    "    print(\"This is expected if credentials are not configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd923d39-b470-4eca-a7c1-3c5df76ef94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading Different File Formats\n",
    "\n",
    "Databricks supports reading various file formats. Let's explore the most common ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ed1100-a897-496b-a4bd-54ee02d3c25d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample data for demonstrations\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"2024-01-01\", \"Product A\", 100.0, 10),\n",
    "    (\"2024-01-02\", \"Product B\", 150.0, 15),\n",
    "    (\"2024-01-03\", \"Product A\", 120.0, 12),\n",
    "    (\"2024-01-04\", \"Product C\", 200.0, 20),\n",
    "    (\"2024-01-05\", \"Product B\", 180.0, 18),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"Sample DataFrame created:\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a5c056-4e67-4f61-9287-242ac37ddbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Reading and Writing CSV Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f66e5c-7e3c-41cd-b485-e70e8954faad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write CSV\n",
    "csv_path = \"/Volumes/workspace/default/databricks_demo/sales_data.csv\"\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "print(f\"CSV written to: {csv_path}\")\n",
    "\n",
    "# Read CSV\n",
    "df_csv = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "print(\"\\nReading CSV:\")\n",
    "df_csv.show()\n",
    "\n",
    "# CSV with custom options\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"{csv_path}_custom\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5720c4-4b6a-477c-85ba-402c63e4df81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Reading and Writing JSON Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38fb532a-9ba4-40d3-85b5-9aa81436dbc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write JSON\n",
    "json_path = \"/Volumes/workspace/default/databricks_demo/sales_data.json\"\n",
    "df.write.mode(\"overwrite\").json(json_path)\n",
    "print(f\"JSON written to: {json_path}\")\n",
    "\n",
    "# Read JSON\n",
    "df_json = spark.read.json(json_path)\n",
    "print(\"\\nReading JSON:\")\n",
    "df_json.show()\n",
    "\n",
    "# JSON with multiline option (for pretty-printed JSON)\n",
    "df.write.mode(\"overwrite\").option(\"multiline\", \"true\").json(f\"{json_path}_multiline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e16fb00-ebdd-4a05-82f3-736f24a9a388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Reading and Writing Parquet Files (Recommended)\n",
    "\n",
    "Parquet is the recommended format for data engineering in Databricks because:\n",
    "- **Columnar storage**: Efficient for analytics queries\n",
    "- **Compression**: Reduces storage costs\n",
    "- **Schema evolution**: Supports schema changes\n",
    "- **Performance**: Optimized for Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65b7f56-a864-4d32-95a6-a16c91835494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Parquet\n",
    "parquet_path = \"/Volumes/workspace/default/databricks_demo/sales_data.parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "print(f\"Parquet written to: {parquet_path}\")\n",
    "\n",
    "# Read Parquet\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "print(\"\\nReading Parquet:\")\n",
    "df_parquet.show()\n",
    "\n",
    "# Parquet with partitioning\n",
    "partitioned_path = \"/Volumes/workspace/default/databricks_demo/sales_data_partitioned\"\n",
    "df.write.mode(\"overwrite\").partitionBy(\"product\").parquet(partitioned_path)\n",
    "print(f\"\\nPartitioned Parquet written to: {partitioned_path}\")\n",
    "\n",
    "# Read partitioned data\n",
    "df_partitioned = spark.read.parquet(partitioned_path)\n",
    "print(\"\\nReading partitioned Parquet:\")\n",
    "df_partitioned.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8c4e15-8ebb-48ac-8e35-4ca57b621e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Reading and Writing Delta Tables\n",
    "\n",
    "Delta Lake is Databricks' optimized storage layer that provides ACID transactions, time travel, and schema enforcement. We'll cover this in detail in Day 4, but here's a quick example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0e8fa9-fbfd-4098-8c8d-4b5903e74172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Delta table\n",
    "delta_path = \"/Volumes/workspace/default/databricks_demo/sales_data_delta\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "print(f\"Delta table written to: {delta_path}\")\n",
    "\n",
    "# Read Delta table\n",
    "df_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "print(\"\\nReading Delta table:\")\n",
    "df_delta.show()\n",
    "\n",
    "# Delta table with partitioning\n",
    "df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"product\").save(f\"{delta_path}_partitioned\")\n",
    "print(f\"\\nPartitioned Delta table written\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa1e5f9-cd75-424f-9c61-dd4ad4335773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Reading Excel Files\n",
    "\n",
    "For Excel files, you'll need to use pandas first, then convert to Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c32e4b1-bf57-4a7e-b9d0-d9753ffdb9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sample Excel file using pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'employee_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'department': ['Engineering', 'Sales', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'salary': [75000, 65000, 80000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Save to Excel (this would typically be uploaded to /FileStore)\n",
    "# For demo, we'll convert directly to Spark\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(pandas_df)\n",
    "\n",
    "# Convert pandas to Spark\n",
    "df_excel = spark.createDataFrame(pandas_df)\n",
    "print(\"\\nConverted to Spark DataFrame:\")\n",
    "df_excel.show()\n",
    "\n",
    "# If you have an Excel file in Volume:\n",
    "# pandas_df = pd.read_excel(\"/dbfs/FileStore/tables/data.xlsx\")\n",
    "# df_excel = spark.createDataFrame(pandas_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82262c10-b05e-435a-bf0d-027e9b8681c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading from Multiple Files and Directories\n",
    "\n",
    "Spark can read from multiple files and directories efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc62e565-91cd-4a1c-9362-4b229a009741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create multiple files for demonstration\n",
    "base_path = \"/Volumes/workspace/default/databricks_demo/multi_file\"\n",
    "\n",
    "# Create data for different dates\n",
    "for i in range(1, 4):\n",
    "    date_data = [\n",
    "        (f\"2024-01-0{i}\", f\"Product A\", 100.0 + i * 10, 10 + i),\n",
    "        (f\"2024-01-0{i}\", f\"Product B\", 150.0 + i * 10, 15 + i),\n",
    "    ]\n",
    "    date_df = spark.createDataFrame(date_data, [\"date\", \"product\", \"amount\", \"quantity\"])\n",
    "    date_df.write.mode(\"overwrite\").parquet(f\"{base_path}/date=2024-01-0{i}\")\n",
    "\n",
    "print(\"Created multiple partitioned files\")\n",
    "\n",
    "# Read all files at once\n",
    "df_all = spark.read.parquet(f\"{base_path}/*\")\n",
    "print(\"\\nReading all files:\")\n",
    "df_all.show()\n",
    "\n",
    "# Read with partition discovery\n",
    "df_partitioned = spark.read.option(\"basePath\", base_path).parquet(f\"{base_path}/date=2024-01-01\", \n",
    "                                                                   f\"{base_path}/date=2024-01-02\")\n",
    "print(\"\\nReading specific partitions:\")\n",
    "df_partitioned.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e794d61-6dde-4bb0-981f-d2a41098383b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with Large Datasets\n",
    "\n",
    "When working with large datasets, consider these best practices:\n",
    "\n",
    "1. **Use Parquet or Delta**: Better compression and performance\n",
    "2. **Partition your data**: Improves query performance\n",
    "3. **Use appropriate file sizes**: 128MB - 1GB per file is optimal\n",
    "4. **Lazy evaluation**: Spark only executes when an action is called\n",
    "5. **Caching**: Cache frequently used DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fd7085-beb9-4be5-abc0-6326c842ba50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a larger dataset for demonstration\n",
    "from pyspark.sql.functions import col, rand, when\n",
    "\n",
    "# Generate 10,000 rows\n",
    "large_df = spark.range(0, 500000).withColumn(\n",
    "    \"value\", rand() * 100\n",
    ").withColumn(\n",
    "    \"category\", \n",
    "    when(rand() < 0.33, \"A\")\n",
    "    .when(rand() < 0.66, \"B\")\n",
    "    .otherwise(\"C\")\n",
    ")\n",
    "\n",
    "print(f\"Created DataFrame with {large_df.count()} rows\")\n",
    "\n",
    "# Write with optimal partitioning\n",
    "output_path = \"/Volumes/workspace/default/databricks_demo/large_dataset\"\n",
    "large_df.write.mode(\"overwrite\").partitionBy(\"category\").parquet(output_path)\n",
    "\n",
    "# Check file sizes\n",
    "print(\"\\nFile sizes in output directory:\")\n",
    "files = dbutils.fs.ls(f\"{output_path}/category=A\")\n",
    "for file in files[:5]:  # Show first 5 files\n",
    "    print(f\"  {file.name}: {file.size / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f92ce69-d95f-4645-9200-240d086439dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices for Data Storage\n",
    "\n",
    "1. **Choose the right format**:\n",
    "   - **Parquet**: For analytics workloads (recommended)\n",
    "   - **Delta**: For ACID transactions and time travel\n",
    "   - **CSV**: Only for small datasets or compatibility\n",
    "   - **JSON**: For semi-structured data\n",
    "\n",
    "2. **Partition your data**:\n",
    "   - Partition by date, region, or other frequently filtered columns\n",
    "   - Avoid over-partitioning (too many small files)\n",
    "   - Aim for 128MB - 1GB per partition\n",
    "\n",
    "3. **File naming**:\n",
    "   - Use descriptive names\n",
    "   - Include date/version in path structure\n",
    "   - Use partitioning instead of file naming for organization\n",
    "\n",
    "4. **Storage location**:\n",
    "   - Use ADLS Gen2 for production data\n",
    "   - Use DBFS /tmp for temporary files\n",
    "   - Use /FileStore for small uploads\n",
    "\n",
    "5. **Security**:\n",
    "   - Use Service Principal for authentication\n",
    "   - Store credentials in Databricks Secret Scopes\n",
    "   - Use mount points for frequently accessed storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f39c66a-1ad7-4d8e-acd9-d2f7747ae6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "✅ **DBFS** - Databricks File System structure and usage\n",
    "\n",
    "✅ **Mounting ADLS Gen2** - How to mount external storage using Service Principal or Access Key\n",
    "\n",
    "✅ **Direct access** - Accessing ADLS Gen2 without mounting\n",
    "\n",
    "✅ **File formats** - Reading and writing CSV, JSON, Parquet, Delta, and Excel files\n",
    "\n",
    "✅ **Multiple files** - Reading from directories and multiple files\n",
    "\n",
    "✅ **Large datasets** - Best practices for working with big data\n",
    "\n",
    "✅ **Storage best practices** - Guidelines for efficient data storage\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- Spark SQL in Databricks\n",
    "- Advanced DataFrame operations\n",
    "- Working with temporary views and global views\n",
    "- Performance optimization techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93dd8b1-d6b6-4d20-9db6-9f012c2dc988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Try these exercises to practice:\n",
    "\n",
    "1. Create a mount point to your ADLS Gen2 storage (if you have access)\n",
    "2. Upload a CSV file to FileStore and read it into a Spark DataFrame\n",
    "3. Create a partitioned Parquet dataset with at least 1000 rows\n",
    "4. Read data from multiple files in a directory\n",
    "5. Compare the file sizes of CSV vs Parquet for the same dataset\n",
    "6. Write a DataFrame to Delta format and read it back\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5296116087534823,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_ingestion_storage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
